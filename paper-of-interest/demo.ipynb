{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class Resnet50FPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet50FPN, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
    "        children = list(self.resnet.children())\n",
    "        self.conv1 = nn.Sequential(*children[:4])\n",
    "        self.conv2 = children[4]\n",
    "        self.conv3 = children[5]\n",
    "        self.conv4 = children[6]\n",
    "    def forward(self, im_data):\n",
    "        feat = OrderedDict()\n",
    "        feat_map = self.conv1(im_data)\n",
    "        feat_map = self.conv2(feat_map)\n",
    "        feat_map3 = self.conv3(feat_map)\n",
    "        feat_map4 = self.conv4(feat_map3)\n",
    "        feat['map3'] = feat_map3\n",
    "        feat['map4'] = feat_map4\n",
    "        return feat\n",
    "\n",
    "\n",
    "class CountRegressor(nn.Module):\n",
    "    def __init__(self, input_channels,pool='mean'):\n",
    "        super(CountRegressor, self).__init__()\n",
    "        self.pool = pool\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 196, 7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(196, 128, 5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        num_sample =  im.shape[0]\n",
    "        if num_sample == 1:\n",
    "            output = self.regressor(im.squeeze(0))\n",
    "            if self.pool == 'mean':\n",
    "                output = torch.mean(output, dim=(0),keepdim=True)  \n",
    "                return output\n",
    "            elif self.pool == 'max':\n",
    "                output, _ = torch.max(output, 0,keepdim=True)\n",
    "                return output\n",
    "        else:\n",
    "            for i in range(0,num_sample):\n",
    "                output = self.regressor(im[i])\n",
    "                if self.pool == 'mean':\n",
    "                    output = torch.mean(output, dim=(0),keepdim=True)\n",
    "                elif self.pool == 'max':\n",
    "                    output, _ = torch.max(output, 0,keepdim=True)\n",
    "                if i == 0:\n",
    "                    Output = output\n",
    "                else:\n",
    "                    Output = torch.cat((Output,output),dim=0)\n",
    "            return Output\n",
    "\n",
    "\n",
    "def weights_normal_init(model, dev=0.01):\n",
    "    if isinstance(model, list):\n",
    "        for m in model:\n",
    "            weights_normal_init(m, dev)\n",
    "    else:\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Conv2d):                \n",
    "                m.weight.data.normal_(0.0, dev)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0.0, dev)\n",
    "\n",
    "\n",
    "def weights_xavier_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "matplotlib.use('agg')\n",
    "\n",
    "\n",
    "MAPS = ['map3','map4']\n",
    "Scales = [0.9, 1.1]\n",
    "MIN_HW = 384\n",
    "MAX_HW = 1584\n",
    "IM_NORM_MEAN = [0.485, 0.456, 0.406]\n",
    "IM_NORM_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def select_exemplar_rois(image):\n",
    "    all_rois = []\n",
    "\n",
    "    print(\"Press 'q' or Esc to quit. Press 'n' and then use mouse drag to draw a new examplar, 'space' to save.\")\n",
    "    while True:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27 or key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('n') or key == '\\r':\n",
    "            rect = cv2.selectROI(\"image\", image, False, False)\n",
    "            x1 = rect[0]\n",
    "            y1 = rect[1]\n",
    "            x2 = x1 + rect[2] - 1\n",
    "            y2 = y1 + rect[3] - 1\n",
    "\n",
    "            all_rois.append([y1, x1, y2, x2])\n",
    "            for rect in all_rois:\n",
    "                y1, x1, y2, x2 = rect\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            print(\"Press q or Esc to quit. Press 'n' and then use mouse drag to draw a new examplar\")\n",
    "\n",
    "    return all_rois\n",
    "\n",
    "def matlab_style_gauss2D(shape=(3,3),sigma=0.5):\n",
    "    \"\"\"\n",
    "    2D gaussian mask - should give the same result as MATLAB's\n",
    "    fspecial('gaussian',[shape],[sigma])\n",
    "    \"\"\"\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n",
    "\n",
    "def PerturbationLoss(output,boxes,sigma=8, use_gpu=True):\n",
    "    Loss = 0.\n",
    "    if boxes.shape[1] > 1:\n",
    "        boxes = boxes.squeeze()\n",
    "        for tempBoxes in boxes.squeeze():\n",
    "            y1 = int(tempBoxes[1])\n",
    "            y2 = int(tempBoxes[3])\n",
    "            x1 = int(tempBoxes[2])\n",
    "            x2 = int(tempBoxes[4])\n",
    "            out = output[:,:,y1:y2,x1:x2]\n",
    "            GaussKernel = matlab_style_gauss2D(shape=(out.shape[2],out.shape[3]),sigma=sigma)\n",
    "            GaussKernel = torch.from_numpy(GaussKernel).float()\n",
    "            if use_gpu: GaussKernel = GaussKernel.cuda()\n",
    "            Loss += F.mse_loss(out.squeeze(),GaussKernel)\n",
    "    else:\n",
    "        boxes = boxes.squeeze()\n",
    "        y1 = int(boxes[1])\n",
    "        y2 = int(boxes[3])\n",
    "        x1 = int(boxes[2])\n",
    "        x2 = int(boxes[4])\n",
    "        out = output[:,:,y1:y2,x1:x2]\n",
    "        Gauss = matlab_style_gauss2D(shape=(out.shape[2],out.shape[3]),sigma=sigma)\n",
    "        GaussKernel = torch.from_numpy(Gauss).float()\n",
    "        if use_gpu: GaussKernel = GaussKernel.cuda()\n",
    "        Loss += F.mse_loss(out.squeeze(),GaussKernel) \n",
    "    return Loss\n",
    "\n",
    "\n",
    "def MincountLoss(output,boxes, use_gpu=True):\n",
    "    ones = torch.ones(1)\n",
    "    if use_gpu: ones = ones.cuda()\n",
    "    Loss = 0.\n",
    "    if boxes.shape[1] > 1:\n",
    "        boxes = boxes.squeeze()\n",
    "        for tempBoxes in boxes.squeeze():\n",
    "            y1 = int(tempBoxes[1])\n",
    "            y2 = int(tempBoxes[3])\n",
    "            x1 = int(tempBoxes[2])\n",
    "            x2 = int(tempBoxes[4])\n",
    "            X = output[:,:,y1:y2,x1:x2].sum()\n",
    "            if X.item() <= 1:\n",
    "                Loss += F.mse_loss(X,ones)\n",
    "    else:\n",
    "        boxes = boxes.squeeze()\n",
    "        y1 = int(boxes[1])\n",
    "        y2 = int(boxes[3])\n",
    "        x1 = int(boxes[2])\n",
    "        x2 = int(boxes[4])\n",
    "        X = output[:,:,y1:y2,x1:x2].sum()\n",
    "        if X.item() <= 1:\n",
    "            Loss += F.mse_loss(X,ones)  \n",
    "    return Loss\n",
    "\n",
    "\n",
    "def pad_to_size(feat, desire_h, desire_w):\n",
    "    \"\"\" zero-padding a four dim feature matrix: N*C*H*W so that the new Height and Width are the desired ones\n",
    "        desire_h and desire_w should be largers than the current height and weight\n",
    "    \"\"\"\n",
    "\n",
    "    cur_h = feat.shape[-2]\n",
    "    cur_w = feat.shape[-1]\n",
    "\n",
    "    left_pad = (desire_w - cur_w + 1) // 2\n",
    "    right_pad = (desire_w - cur_w) - left_pad\n",
    "    top_pad = (desire_h - cur_h + 1) // 2\n",
    "    bottom_pad =(desire_h - cur_h) - top_pad\n",
    "\n",
    "    return F.pad(feat, (left_pad, right_pad, top_pad, bottom_pad))\n",
    "\n",
    "\n",
    "def extract_features(feature_model, image, boxes,feat_map_keys=['map3','map4'], exemplar_scales=[0.9, 1.1]):\n",
    "    N, M = image.shape[0], boxes.shape[2]\n",
    "    \"\"\"\n",
    "    Getting features for the image N * C * H * W\n",
    "    \"\"\"\n",
    "    Image_features = feature_model(image)\n",
    "    \"\"\"\n",
    "    Getting features for the examples (N*M) * C * h * w\n",
    "    \"\"\"\n",
    "    for ix in range(0,N):\n",
    "        # boxes = boxes.squeeze(0)\n",
    "        boxes = boxes[ix][0]\n",
    "        cnter = 0\n",
    "        Cnter1 = 0\n",
    "        for keys in feat_map_keys:\n",
    "            image_features = Image_features[keys][ix].unsqueeze(0)\n",
    "            if keys == 'map1' or keys == 'map2':\n",
    "                Scaling = 4.0\n",
    "            elif keys == 'map3':\n",
    "                Scaling = 8.0\n",
    "            elif keys == 'map4':\n",
    "                Scaling =  16.0\n",
    "            else:\n",
    "                Scaling = 32.0\n",
    "            boxes_scaled = boxes / Scaling\n",
    "            boxes_scaled[:, 1:3] = torch.floor(boxes_scaled[:, 1:3])\n",
    "            boxes_scaled[:, 3:5] = torch.ceil(boxes_scaled[:, 3:5])\n",
    "            boxes_scaled[:, 3:5] = boxes_scaled[:, 3:5] + 1 # make the end indices exclusive \n",
    "            feat_h, feat_w = image_features.shape[-2], image_features.shape[-1]\n",
    "            # make sure exemplars don't go out of bound\n",
    "            boxes_scaled[:, 1:3] = torch.clamp_min(boxes_scaled[:, 1:3], 0)\n",
    "            boxes_scaled[:, 3] = torch.clamp_max(boxes_scaled[:, 3], feat_h)\n",
    "            boxes_scaled[:, 4] = torch.clamp_max(boxes_scaled[:, 4], feat_w)            \n",
    "            box_hs = boxes_scaled[:, 3] - boxes_scaled[:, 1]\n",
    "            box_ws = boxes_scaled[:, 4] - boxes_scaled[:, 2]            \n",
    "            max_h = math.ceil(max(box_hs))\n",
    "            max_w = math.ceil(max(box_ws))            \n",
    "            for j in range(0,M):\n",
    "                y1, x1 = int(boxes_scaled[j,1]), int(boxes_scaled[j,2])  \n",
    "                y2, x2 = int(boxes_scaled[j,3]), int(boxes_scaled[j,4]) \n",
    "                #print(y1,y2,x1,x2,max_h,max_w)\n",
    "                if j == 0:\n",
    "                    examples_features = image_features[:,:,y1:y2, x1:x2]\n",
    "                    if examples_features.shape[2] != max_h or examples_features.shape[3] != max_w:\n",
    "                        #examples_features = pad_to_size(examples_features, max_h, max_w)\n",
    "                        examples_features = F.interpolate(examples_features, size=(max_h,max_w),mode='bilinear')                    \n",
    "                else:\n",
    "                    feat = image_features[:,:,y1:y2, x1:x2]\n",
    "                    if feat.shape[2] != max_h or feat.shape[3] != max_w:\n",
    "                        feat = F.interpolate(feat, size=(max_h,max_w),mode='bilinear')\n",
    "                        #feat = pad_to_size(feat, max_h, max_w)\n",
    "                    examples_features = torch.cat((examples_features,feat),dim=0)\n",
    "            \"\"\"\n",
    "            Convolving example features over image features\n",
    "            \"\"\"\n",
    "            h, w = examples_features.shape[2], examples_features.shape[3]\n",
    "            features =    F.conv2d(\n",
    "                    F.pad(image_features, ((int(w/2)), int((w-1)/2), int(h/2), int((h-1)/2))),\n",
    "                    examples_features\n",
    "                )\n",
    "            combined = features.permute([1,0,2,3])\n",
    "            # computing features for scales 0.9 and 1.1 \n",
    "            for scale in exemplar_scales:\n",
    "                    h1 = math.ceil(h * scale)\n",
    "                    w1 = math.ceil(w * scale)\n",
    "                    if h1 < 1: # use original size if scaled size is too small\n",
    "                        h1 = h\n",
    "                    if w1 < 1:\n",
    "                        w1 = w\n",
    "                    examples_features_scaled = F.interpolate(examples_features, size=(h1,w1),mode='bilinear')  \n",
    "                    features_scaled =    F.conv2d(F.pad(image_features, ((int(w1/2)), int((w1-1)/2), int(h1/2), int((h1-1)/2))),\n",
    "                    examples_features_scaled)\n",
    "                    features_scaled = features_scaled.permute([1,0,2,3])\n",
    "                    combined = torch.cat((combined,features_scaled),dim=1)\n",
    "            if cnter == 0:\n",
    "                Combined = 1.0 * combined\n",
    "            else:\n",
    "                if Combined.shape[2] != combined.shape[2] or Combined.shape[3] != combined.shape[3]:\n",
    "                    combined = F.interpolate(combined, size=(Combined.shape[2],Combined.shape[3]),mode='bilinear')\n",
    "                Combined = torch.cat((Combined,combined),dim=1)\n",
    "            cnter += 1\n",
    "        if ix == 0:\n",
    "            All_feat = 1.0 * Combined.unsqueeze(0)\n",
    "        else:\n",
    "            All_feat = torch.cat((All_feat,Combined.unsqueeze(0)),dim=0)\n",
    "    return All_feat\n",
    "\n",
    "\n",
    "class resizeImage(object):\n",
    "    \"\"\"\n",
    "    If either the width or height of an image exceed a specified value, resize the image so that:\n",
    "        1. The maximum of the new height and new width does not exceed a specified value\n",
    "        2. The new height and new width are divisible by 8\n",
    "        3. The aspect ratio is preserved\n",
    "    No resizing is done if both height and width are smaller than the specified value\n",
    "    By: Minh Hoai Nguyen (minhhoai@gmail.com)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, MAX_HW=1504):\n",
    "        self.max_hw = MAX_HW\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image,lines_boxes = sample['image'], sample['lines_boxes']\n",
    "        \n",
    "        W, H = image.size\n",
    "        if W > self.max_hw or H > self.max_hw:\n",
    "            scale_factor = float(self.max_hw)/ max(H, W)\n",
    "            new_H = 8*int(H*scale_factor/8)\n",
    "            new_W = 8*int(W*scale_factor/8)\n",
    "            resized_image = transforms.Resize((new_H, new_W))(image)\n",
    "        else:\n",
    "            scale_factor = 1\n",
    "            resized_image = image\n",
    "\n",
    "        boxes = list()\n",
    "        for box in lines_boxes:\n",
    "            box2 = [int(k*scale_factor) for k in box]\n",
    "            y1, x1, y2, x2 = box2[0], box2[1], box2[2], box2[3]\n",
    "            boxes.append([0, y1,x1,y2,x2])\n",
    "\n",
    "        boxes = torch.Tensor(boxes).unsqueeze(0)\n",
    "        resized_image = Normalize(resized_image)\n",
    "        sample = {'image':resized_image,'boxes':boxes}\n",
    "        return sample\n",
    "\n",
    "\n",
    "class resizeImageWithGT(object):\n",
    "    \"\"\"\n",
    "    If either the width or height of an image exceed a specified value, resize the image so that:\n",
    "        1. The maximum of the new height and new width does not exceed a specified value\n",
    "        2. The new height and new width are divisible by 8\n",
    "        3. The aspect ratio is preserved\n",
    "    No resizing is done if both height and width are smaller than the specified value\n",
    "    By: Minh Hoai Nguyen (minhhoai@gmail.com)\n",
    "    Modified by: Viresh\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, MAX_HW=1504):\n",
    "        self.max_hw = MAX_HW\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image,lines_boxes,density = sample['image'], sample['lines_boxes'],sample['gt_density']\n",
    "        \n",
    "        W, H = image.size\n",
    "        if W > self.max_hw or H > self.max_hw:\n",
    "            scale_factor = float(self.max_hw)/ max(H, W)\n",
    "            new_H = 8*int(H*scale_factor/8)\n",
    "            new_W = 8*int(W*scale_factor/8)\n",
    "            resized_image = transforms.Resize((new_H, new_W))(image)\n",
    "            resized_density = cv2.resize(density, (new_W, new_H))\n",
    "            orig_count = np.sum(density)\n",
    "            new_count = np.sum(resized_density)\n",
    "\n",
    "            if new_count > 0: resized_density = resized_density * (orig_count / new_count)\n",
    "            \n",
    "        else:\n",
    "            scale_factor = 1\n",
    "            resized_image = image\n",
    "            resized_density = density\n",
    "        boxes = list()\n",
    "        for box in lines_boxes:\n",
    "            box2 = [int(k*scale_factor) for k in box]\n",
    "            y1, x1, y2, x2 = box2[0], box2[1], box2[2], box2[3]\n",
    "            boxes.append([0, y1,x1,y2,x2])\n",
    "\n",
    "        boxes = torch.Tensor(boxes).unsqueeze(0)\n",
    "        resized_image = Normalize(resized_image)\n",
    "        resized_density = torch.from_numpy(resized_density).unsqueeze(0).unsqueeze(0)\n",
    "        sample = {'image':resized_image,'boxes':boxes,'gt_density':resized_density}\n",
    "        return sample\n",
    "\n",
    "\n",
    "Normalize = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IM_NORM_MEAN, std=IM_NORM_STD)])\n",
    "Transform = transforms.Compose([resizeImage( MAX_HW)])\n",
    "TransformTrain = transforms.Compose([resizeImageWithGT(MAX_HW)])\n",
    "\n",
    "\n",
    "def denormalize(tensor, means=IM_NORM_MEAN, stds=IM_NORM_STD):\n",
    "    \"\"\"Reverses the normalisation on a tensor.\n",
    "    Performs a reverse operation on a tensor, so the pixel value range is\n",
    "    between 0 and 1. Useful for when plotting a tensor into an image.\n",
    "    Normalisation: (image - mean) / std\n",
    "    Denormalisation: image * std + mean\n",
    "    Args:\n",
    "        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n",
    "    Shape:\n",
    "        Input: :math:`(N, C, H, W)`\n",
    "        Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Return:\n",
    "        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n",
    "            values between [0, 1]\n",
    "    Note:\n",
    "        Symbols used to describe dimensions:\n",
    "            - N: number of images in a batch\n",
    "            - C: number of channels\n",
    "            - H: height of the image\n",
    "            - W: width of the image\n",
    "    \"\"\"\n",
    "\n",
    "    denormalized = tensor.clone()\n",
    "\n",
    "    for channel, mean, std in zip(denormalized, means, stds):\n",
    "        channel.mul_(std).add_(mean)\n",
    "\n",
    "    return denormalized\n",
    "\n",
    "\n",
    "def scale_and_clip(val, scale_factor, min_val, max_val):\n",
    "    \"Helper function to scale a value and clip it within range\"\n",
    "\n",
    "    new_val = int(round(val*scale_factor))\n",
    "    new_val = max(new_val, min_val)\n",
    "    new_val = min(new_val, max_val)\n",
    "    return new_val\n",
    "\n",
    "\n",
    "def visualize_output_and_save(input_, output, boxes, save_path, figsize=(20, 12), dots=None):\n",
    "    \"\"\"\n",
    "        dots: Nx2 numpy array for the ground truth locations of the dot annotation\n",
    "            if dots is None, this information is not available\n",
    "    \"\"\"\n",
    "\n",
    "    # get the total count\n",
    "    pred_cnt = output.sum().item()\n",
    "    boxes = boxes.squeeze(0)\n",
    "\n",
    "    boxes2 = []\n",
    "    for i in range(0, boxes.shape[0]):\n",
    "        y1, x1, y2, x2 = int(boxes[i, 1].item()), int(boxes[i, 2].item()), int(boxes[i, 3].item()), int(\n",
    "            boxes[i, 4].item())\n",
    "        roi_cnt = output[0,0,y1:y2, x1:x2].sum().item()\n",
    "        boxes2.append([y1, x1, y2, x2, roi_cnt])\n",
    "\n",
    "    img1 = format_for_plotting(denormalize(input_))\n",
    "    output = format_for_plotting(output)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # display the input image\n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(img1)\n",
    "\n",
    "    for bbox in boxes2:\n",
    "        y1, x1, y2, x2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=3, edgecolor='y', facecolor='none')\n",
    "        rect2 = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='k', linestyle='--', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.add_patch(rect2)\n",
    "\n",
    "    if dots is not None:\n",
    "        ax.scatter(dots[:, 0], dots[:, 1], c='red', edgecolors='blue')\n",
    "        # ax.scatter(dots[:,0], dots[:,1], c='black', marker='+')\n",
    "        ax.set_title(\"Input image, gt count: {}\".format(dots.shape[0]))\n",
    "    else:\n",
    "        ax.set_title(\"Input image\")\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Overlaid result, predicted count: {:.2f}\".format(pred_cnt))\n",
    "\n",
    "    img2 = 0.2989*img1[:,:,0] + 0.5870*img1[:,:,1] + 0.1140*img1[:,:,2]\n",
    "    ax.imshow(img2, cmap='gray')\n",
    "    ax.imshow(output, cmap=plt.cm.viridis, alpha=0.5)\n",
    "\n",
    "\n",
    "    # display the density map\n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Density map, predicted count: {:.2f}\".format(pred_cnt))\n",
    "    ax.imshow(output)\n",
    "    # plt.colorbar()\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 4)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Density map, predicted count: {:.2f}\".format(pred_cnt))\n",
    "    ret_fig = ax.imshow(output)\n",
    "    for bbox in boxes2:\n",
    "        y1, x1, y2, x2, roi_cnt = bbox[0], bbox[1], bbox[2], bbox[3], bbox[4]\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=3, edgecolor='y', facecolor='none')\n",
    "        rect2 = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='k', linestyle='--',\n",
    "                                  facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.add_patch(rect2)\n",
    "        ax.text(x1, y1, '{:.2f}'.format(roi_cnt), backgroundcolor='y')\n",
    "\n",
    "    fig.colorbar(ret_fig, ax=ax)\n",
    "\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def format_for_plotting(tensor):\n",
    "    \"\"\"Formats the shape of tensor for plotting.\n",
    "    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
    "    which is not suitable for plotting as images. This function formats an\n",
    "    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n",
    "    data.\n",
    "    Args:\n",
    "        tensor (torch.Tensor, torch.float32): Image tensor\n",
    "    Shape:\n",
    "        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
    "        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n",
    "    Return:\n",
    "        torch.Tensor (torch.float32): Formatted image tensor (detached)\n",
    "    Note:\n",
    "        Symbols used to describe dimensions:\n",
    "            - N: number of images in a batch\n",
    "            - C: number of channels\n",
    "            - H: height of the image\n",
    "            - W: width of the image\n",
    "    \"\"\"\n",
    "\n",
    "    has_batch_dimension = len(tensor.shape) == 4\n",
    "    formatted = tensor.clone()\n",
    "\n",
    "    if has_batch_dimension:\n",
    "        formatted = tensor.squeeze(0)\n",
    "\n",
    "    if formatted.shape[0] == 1:\n",
    "        return formatted.squeeze(0).detach()\n",
    "    else:\n",
    "        return formatted.permute(1, 2, 0).detach()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -i ORANGE.JPG [-b BBOX_FILE] [-o OUTPUT_DIR]\n",
      "                             [-m MODEL_PATH] [-g GPU_ID] [-a]\n",
      "                             [-gs GRADIENT_STEPS] [-lr LEARNING_RATE]\n",
      "                             [-wm WEIGHT_MINCOUNT] [-wp WEIGHT_PERTURBATION]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -i/--orange.jpg\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demo file for Few Shot Counting\n",
    "\n",
    "By: Minh Hoai Nguyen (minhhoai@cs.stonybrook.edu)\n",
    "Created: 19-Apr-2021\n",
    "Last modified: 19-Apr-2021\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "from model import CountRegressor, Resnet50FPN\n",
    "from utils import MAPS, Scales, Transform, extract_features\n",
    "from utils import visualize_output_and_save, select_exemplar_rois\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "from utils import MincountLoss, PerturbationLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Few Shot Counting Demo code\")\n",
    "parser.add_argument(\"-i\", \"--orange.jpg\", type=str, required=True, help=\"/Path/to/input/image/file/\")\n",
    "parser.add_argument(\"-b\", \"--bbox-file\", type=str, help=\"/Path/to/file/of/bounding/boxes\")\n",
    "parser.add_argument(\"-o\", \"--output-dir\", type=str, default=\".\", help=\"/Path/to/output/image/file\")\n",
    "parser.add_argument(\"-m\",  \"--model_path\", type=str, default=\"./data/pretrainedModels/FamNet_Save1.pth\", help=\"path to trained model\")\n",
    "parser.add_argument(\"-g\",  \"--gpu-id\", type=int, default=0, help=\"GPU id. Default 0 for the first GPU. Use -1 for CPU.\")\n",
    "\n",
    "parser.add_argument(\"-a\",  \"--adapt\", action='store_true', help=\"If specified, perform test time adaptation\")\n",
    "parser.add_argument(\"-gs\", \"--gradient_steps\", type=int,default=100, help=\"number of gradient steps for the adaptation\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float,default=1e-7, help=\"learning rate for adaptation\")\n",
    "parser.add_argument(\"-wm\", \"--weight_mincount\", type=float,default=1e-9, help=\"weight multiplier for Mincount Loss\")\n",
    "parser.add_argument(\"-wp\", \"--weight_perturbation\", type=float,default=1e-4, help=\"weight multiplier for Perturbation Loss\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not torch.cuda.is_available() or args.gpu_id < 0:\n",
    "    use_gpu = False\n",
    "    print(\"===> Using CPU mode.\")\n",
    "else:\n",
    "    use_gpu = True\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "\n",
    "resnet50_conv = Resnet50FPN()\n",
    "regressor = CountRegressor(6, pool='mean')\n",
    "\n",
    "if use_gpu:\n",
    "    resnet50_conv.cuda()\n",
    "    regressor.cuda()\n",
    "    regressor.load_state_dict(torch.load(args.model_path))\n",
    "else:\n",
    "    regressor.load_state_dict(torch.load(args.model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "resnet50_conv.eval()\n",
    "regressor.eval()\n",
    "\n",
    "image_name = os.path.basename(args.input_image)\n",
    "image_name = os.path.splitext(image_name)[0]\n",
    "\n",
    "if args.bbox_file is None: # if no bounding box file is given, prompt the user for a set of bounding boxes\n",
    "    out_bbox_file = \"{}/{}_box.txt\".format(args.output_dir, image_name)\n",
    "    fout = open(out_bbox_file, \"w\")\n",
    "\n",
    "    im = cv2.imread(args.input_image)\n",
    "    cv2.imshow('image', im)\n",
    "    rects = select_exemplar_rois(im)\n",
    "\n",
    "    rects1 = list()\n",
    "    for rect in rects:\n",
    "        y1, x1, y2, x2 = rect\n",
    "        rects1.append([y1, x1, y2, x2])\n",
    "        fout.write(\"{} {} {} {}\\n\".format(y1, x1, y2, x2))\n",
    "\n",
    "    fout.close()\n",
    "    cv2.destroyWindow(\"Image\")\n",
    "    print(\"selected bounding boxes are saved to {}\".format(out_bbox_file))\n",
    "else:\n",
    "    with open(args.bbox_file, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "\n",
    "    rects1 = list()\n",
    "    for line in lines:\n",
    "        data = line.split()\n",
    "        y1 = int(data[0])\n",
    "        x1 = int(data[1])\n",
    "        y2 = int(data[2])\n",
    "        x2 = int(data[3])\n",
    "        rects1.append([y1, x1, y2, x2])\n",
    "\n",
    "#print(\"Bounding boxes: \", end=\"\")\n",
    "print(rects1)\n",
    "\n",
    "image = Image.open(args.input_image)\n",
    "image.load()\n",
    "sample = {'image': image, 'lines_boxes': rects1}\n",
    "sample = Transform(sample)\n",
    "image, boxes = sample['image'], sample['boxes']\n",
    "\n",
    "\n",
    "if use_gpu:\n",
    "    image = image.cuda()\n",
    "    boxes = boxes.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = extract_features(resnet50_conv, image.unsqueeze(0), boxes.unsqueeze(0), MAPS, Scales)\n",
    "\n",
    "if not args.adapt:\n",
    "    with torch.no_grad(): output = regressor(features)\n",
    "else:\n",
    "    features.required_grad = True\n",
    "    #adapted_regressor = copy.deepcopy(regressor)\n",
    "    adapted_regressor = regressor\n",
    "    adapted_regressor.train()\n",
    "    optimizer = optim.Adam(adapted_regressor.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    pbar = tqdm(range(args.gradient_steps))\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = adapted_regressor(features)\n",
    "        lCount = args.weight_mincount * MincountLoss(output, boxes, use_gpu=use_gpu)\n",
    "        lPerturbation = args.weight_perturbation * PerturbationLoss(output, boxes, sigma=8, use_gpu=use_gpu)\n",
    "        Loss = lCount + lPerturbation\n",
    "        # loss can become zero in some cases, where loss is a 0 valued scalar and not a tensor\n",
    "        # So Perform gradient descent only for non zero cases\n",
    "        if torch.is_tensor(Loss):\n",
    "            Loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        pbar.set_description('Adaptation step: {:<3}, loss: {}, predicted-count: {:6.1f}'.format(step, Loss.item(), output.sum().item()))\n",
    "\n",
    "    features.required_grad = False\n",
    "    output = adapted_regressor(features)\n",
    "\n",
    "\n",
    "print('===> The predicted count is: {:6.2f}'.format(output.sum().item()))\n",
    "\n",
    "rslt_file = \"{}/{}_out.png\".format(args.output_dir, image_name)\n",
    "visualize_output_and_save(image.detach().cpu(), output.detach().cpu(), boxes.cpu(), rslt_file)\n",
    "print(\"===> Visualized output is saved to {}\".format(rslt_file))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
