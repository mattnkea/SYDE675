{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "matplotlib.use('agg')\n",
    "\n",
    "\n",
    "MAPS = ['map3','map4']\n",
    "Scales = [0.9, 1.1]\n",
    "MIN_HW = 384\n",
    "MAX_HW = 1584\n",
    "IM_NORM_MEAN = [0.485, 0.456, 0.406]\n",
    "IM_NORM_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def select_exemplar_rois(image):\n",
    "    all_rois = []\n",
    "\n",
    "    print(\"Press 'q' or Esc to quit. Press 'n' and then use mouse drag to draw a new examplar, 'space' to save.\")\n",
    "    while True:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27 or key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('n') or key == '\\r':\n",
    "            rect = cv2.selectROI(\"image\", image, False, False)\n",
    "            x1 = rect[0]\n",
    "            y1 = rect[1]\n",
    "            x2 = x1 + rect[2] - 1\n",
    "            y2 = y1 + rect[3] - 1\n",
    "\n",
    "            all_rois.append([y1, x1, y2, x2])\n",
    "            for rect in all_rois:\n",
    "                y1, x1, y2, x2 = rect\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            print(\"Press q or Esc to quit. Press 'n' and then use mouse drag to draw a new examplar\")\n",
    "\n",
    "    return all_rois\n",
    "\n",
    "def matlab_style_gauss2D(shape=(3,3),sigma=0.5):\n",
    "    \"\"\"\n",
    "    2D gaussian mask - should give the same result as MATLAB's\n",
    "    fspecial('gaussian',[shape],[sigma])\n",
    "    \"\"\"\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n",
    "\n",
    "def PerturbationLoss(output,boxes,sigma=8, use_gpu=True):\n",
    "    Loss = 0.\n",
    "    if boxes.shape[1] > 1:\n",
    "        boxes = boxes.squeeze()\n",
    "        for tempBoxes in boxes.squeeze():\n",
    "            y1 = int(tempBoxes[1])\n",
    "            y2 = int(tempBoxes[3])\n",
    "            x1 = int(tempBoxes[2])\n",
    "            x2 = int(tempBoxes[4])\n",
    "            out = output[:,:,y1:y2,x1:x2]\n",
    "            GaussKernel = matlab_style_gauss2D(shape=(out.shape[2],out.shape[3]),sigma=sigma)\n",
    "            GaussKernel = torch.from_numpy(GaussKernel).float()\n",
    "            if use_gpu: GaussKernel = GaussKernel.cuda()\n",
    "            Loss += F.mse_loss(out.squeeze(),GaussKernel)\n",
    "    else:\n",
    "        boxes = boxes.squeeze()\n",
    "        y1 = int(boxes[1])\n",
    "        y2 = int(boxes[3])\n",
    "        x1 = int(boxes[2])\n",
    "        x2 = int(boxes[4])\n",
    "        out = output[:,:,y1:y2,x1:x2]\n",
    "        Gauss = matlab_style_gauss2D(shape=(out.shape[2],out.shape[3]),sigma=sigma)\n",
    "        GaussKernel = torch.from_numpy(Gauss).float()\n",
    "        if use_gpu: GaussKernel = GaussKernel.cuda()\n",
    "        Loss += F.mse_loss(out.squeeze(),GaussKernel) \n",
    "    return Loss\n",
    "\n",
    "\n",
    "def MincountLoss(output,boxes, use_gpu=True):\n",
    "    ones = torch.ones(1)\n",
    "    if use_gpu: ones = ones.cuda()\n",
    "    Loss = 0.\n",
    "    if boxes.shape[1] > 1:\n",
    "        boxes = boxes.squeeze()\n",
    "        for tempBoxes in boxes.squeeze():\n",
    "            y1 = int(tempBoxes[1])\n",
    "            y2 = int(tempBoxes[3])\n",
    "            x1 = int(tempBoxes[2])\n",
    "            x2 = int(tempBoxes[4])\n",
    "            X = output[:,:,y1:y2,x1:x2].sum()\n",
    "            if X.item() <= 1:\n",
    "                Loss += F.mse_loss(X,ones)\n",
    "    else:\n",
    "        boxes = boxes.squeeze()\n",
    "        y1 = int(boxes[1])\n",
    "        y2 = int(boxes[3])\n",
    "        x1 = int(boxes[2])\n",
    "        x2 = int(boxes[4])\n",
    "        X = output[:,:,y1:y2,x1:x2].sum()\n",
    "        if X.item() <= 1:\n",
    "            Loss += F.mse_loss(X,ones)  \n",
    "    return Loss\n",
    "\n",
    "\n",
    "def pad_to_size(feat, desire_h, desire_w):\n",
    "    \"\"\" zero-padding a four dim feature matrix: N*C*H*W so that the new Height and Width are the desired ones\n",
    "        desire_h and desire_w should be largers than the current height and weight\n",
    "    \"\"\"\n",
    "\n",
    "    cur_h = feat.shape[-2]\n",
    "    cur_w = feat.shape[-1]\n",
    "\n",
    "    left_pad = (desire_w - cur_w + 1) // 2\n",
    "    right_pad = (desire_w - cur_w) - left_pad\n",
    "    top_pad = (desire_h - cur_h + 1) // 2\n",
    "    bottom_pad =(desire_h - cur_h) - top_pad\n",
    "\n",
    "    return F.pad(feat, (left_pad, right_pad, top_pad, bottom_pad))\n",
    "\n",
    "\n",
    "def extract_features(feature_model, image, boxes,feat_map_keys=['map3','map4'], exemplar_scales=[0.9, 1.1]):\n",
    "    N, M = image.shape[0], boxes.shape[2]\n",
    "    \"\"\"\n",
    "    Getting features for the image N * C * H * W\n",
    "    \"\"\"\n",
    "    Image_features = feature_model(image)\n",
    "    \"\"\"\n",
    "    Getting features for the examples (N*M) * C * h * w\n",
    "    \"\"\"\n",
    "    for ix in range(0,N):\n",
    "        # boxes = boxes.squeeze(0)\n",
    "        boxes = boxes[ix][0]\n",
    "        cnter = 0\n",
    "        Cnter1 = 0\n",
    "        for keys in feat_map_keys:\n",
    "            image_features = Image_features[keys][ix].unsqueeze(0)\n",
    "            if keys == 'map1' or keys == 'map2':\n",
    "                Scaling = 4.0\n",
    "            elif keys == 'map3':\n",
    "                Scaling = 8.0\n",
    "            elif keys == 'map4':\n",
    "                Scaling =  16.0\n",
    "            else:\n",
    "                Scaling = 32.0\n",
    "            boxes_scaled = boxes / Scaling\n",
    "            boxes_scaled[:, 1:3] = torch.floor(boxes_scaled[:, 1:3])\n",
    "            boxes_scaled[:, 3:5] = torch.ceil(boxes_scaled[:, 3:5])\n",
    "            boxes_scaled[:, 3:5] = boxes_scaled[:, 3:5] + 1 # make the end indices exclusive \n",
    "            feat_h, feat_w = image_features.shape[-2], image_features.shape[-1]\n",
    "            # make sure exemplars don't go out of bound\n",
    "            boxes_scaled[:, 1:3] = torch.clamp_min(boxes_scaled[:, 1:3], 0)\n",
    "            boxes_scaled[:, 3] = torch.clamp_max(boxes_scaled[:, 3], feat_h)\n",
    "            boxes_scaled[:, 4] = torch.clamp_max(boxes_scaled[:, 4], feat_w)            \n",
    "            box_hs = boxes_scaled[:, 3] - boxes_scaled[:, 1]\n",
    "            box_ws = boxes_scaled[:, 4] - boxes_scaled[:, 2]            \n",
    "            max_h = math.ceil(max(box_hs))\n",
    "            max_w = math.ceil(max(box_ws))            \n",
    "            for j in range(0,M):\n",
    "                y1, x1 = int(boxes_scaled[j,1]), int(boxes_scaled[j,2])  \n",
    "                y2, x2 = int(boxes_scaled[j,3]), int(boxes_scaled[j,4]) \n",
    "                #print(y1,y2,x1,x2,max_h,max_w)\n",
    "                if j == 0:\n",
    "                    examples_features = image_features[:,:,y1:y2, x1:x2]\n",
    "                    if examples_features.shape[2] != max_h or examples_features.shape[3] != max_w:\n",
    "                        #examples_features = pad_to_size(examples_features, max_h, max_w)\n",
    "                        examples_features = F.interpolate(examples_features, size=(max_h,max_w),mode='bilinear')                    \n",
    "                else:\n",
    "                    feat = image_features[:,:,y1:y2, x1:x2]\n",
    "                    if feat.shape[2] != max_h or feat.shape[3] != max_w:\n",
    "                        feat = F.interpolate(feat, size=(max_h,max_w),mode='bilinear')\n",
    "                        #feat = pad_to_size(feat, max_h, max_w)\n",
    "                    examples_features = torch.cat((examples_features,feat),dim=0)\n",
    "            \"\"\"\n",
    "            Convolving example features over image features\n",
    "            \"\"\"\n",
    "            h, w = examples_features.shape[2], examples_features.shape[3]\n",
    "            features =    F.conv2d(\n",
    "                    F.pad(image_features, ((int(w/2)), int((w-1)/2), int(h/2), int((h-1)/2))),\n",
    "                    examples_features\n",
    "                )\n",
    "            combined = features.permute([1,0,2,3])\n",
    "            # computing features for scales 0.9 and 1.1 \n",
    "            for scale in exemplar_scales:\n",
    "                    h1 = math.ceil(h * scale)\n",
    "                    w1 = math.ceil(w * scale)\n",
    "                    if h1 < 1: # use original size if scaled size is too small\n",
    "                        h1 = h\n",
    "                    if w1 < 1:\n",
    "                        w1 = w\n",
    "                    examples_features_scaled = F.interpolate(examples_features, size=(h1,w1),mode='bilinear')  \n",
    "                    features_scaled =    F.conv2d(F.pad(image_features, ((int(w1/2)), int((w1-1)/2), int(h1/2), int((h1-1)/2))),\n",
    "                    examples_features_scaled)\n",
    "                    features_scaled = features_scaled.permute([1,0,2,3])\n",
    "                    combined = torch.cat((combined,features_scaled),dim=1)\n",
    "            if cnter == 0:\n",
    "                Combined = 1.0 * combined\n",
    "            else:\n",
    "                if Combined.shape[2] != combined.shape[2] or Combined.shape[3] != combined.shape[3]:\n",
    "                    combined = F.interpolate(combined, size=(Combined.shape[2],Combined.shape[3]),mode='bilinear')\n",
    "                Combined = torch.cat((Combined,combined),dim=1)\n",
    "            cnter += 1\n",
    "        if ix == 0:\n",
    "            All_feat = 1.0 * Combined.unsqueeze(0)\n",
    "        else:\n",
    "            All_feat = torch.cat((All_feat,Combined.unsqueeze(0)),dim=0)\n",
    "    return All_feat\n",
    "\n",
    "\n",
    "class resizeImage(object):\n",
    "    \"\"\"\n",
    "    If either the width or height of an image exceed a specified value, resize the image so that:\n",
    "        1. The maximum of the new height and new width does not exceed a specified value\n",
    "        2. The new height and new width are divisible by 8\n",
    "        3. The aspect ratio is preserved\n",
    "    No resizing is done if both height and width are smaller than the specified value\n",
    "    By: Minh Hoai Nguyen (minhhoai@gmail.com)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, MAX_HW=1504):\n",
    "        self.max_hw = MAX_HW\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image,lines_boxes = sample['image'], sample['lines_boxes']\n",
    "        \n",
    "        W, H = image.size\n",
    "        if W > self.max_hw or H > self.max_hw:\n",
    "            scale_factor = float(self.max_hw)/ max(H, W)\n",
    "            new_H = 8*int(H*scale_factor/8)\n",
    "            new_W = 8*int(W*scale_factor/8)\n",
    "            resized_image = transforms.Resize((new_H, new_W))(image)\n",
    "        else:\n",
    "            scale_factor = 1\n",
    "            resized_image = image\n",
    "\n",
    "        boxes = list()\n",
    "        for box in lines_boxes:\n",
    "            box2 = [int(k*scale_factor) for k in box]\n",
    "            y1, x1, y2, x2 = box2[0], box2[1], box2[2], box2[3]\n",
    "            boxes.append([0, y1,x1,y2,x2])\n",
    "\n",
    "        boxes = torch.Tensor(boxes).unsqueeze(0)\n",
    "        resized_image = Normalize(resized_image)\n",
    "        sample = {'image':resized_image,'boxes':boxes}\n",
    "        return sample\n",
    "\n",
    "\n",
    "class resizeImageWithGT(object):\n",
    "    \"\"\"\n",
    "    If either the width or height of an image exceed a specified value, resize the image so that:\n",
    "        1. The maximum of the new height and new width does not exceed a specified value\n",
    "        2. The new height and new width are divisible by 8\n",
    "        3. The aspect ratio is preserved\n",
    "    No resizing is done if both height and width are smaller than the specified value\n",
    "    By: Minh Hoai Nguyen (minhhoai@gmail.com)\n",
    "    Modified by: Viresh\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, MAX_HW=1504):\n",
    "        self.max_hw = MAX_HW\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image,lines_boxes,density = sample['image'], sample['lines_boxes'],sample['gt_density']\n",
    "        \n",
    "        W, H = image.size\n",
    "        if W > self.max_hw or H > self.max_hw:\n",
    "            scale_factor = float(self.max_hw)/ max(H, W)\n",
    "            new_H = 8*int(H*scale_factor/8)\n",
    "            new_W = 8*int(W*scale_factor/8)\n",
    "            resized_image = transforms.Resize((new_H, new_W))(image)\n",
    "            resized_density = cv2.resize(density, (new_W, new_H))\n",
    "            orig_count = np.sum(density)\n",
    "            new_count = np.sum(resized_density)\n",
    "\n",
    "            if new_count > 0: resized_density = resized_density * (orig_count / new_count)\n",
    "            \n",
    "        else:\n",
    "            scale_factor = 1\n",
    "            resized_image = image\n",
    "            resized_density = density\n",
    "        boxes = list()\n",
    "        for box in lines_boxes:\n",
    "            box2 = [int(k*scale_factor) for k in box]\n",
    "            y1, x1, y2, x2 = box2[0], box2[1], box2[2], box2[3]\n",
    "            boxes.append([0, y1,x1,y2,x2])\n",
    "\n",
    "        boxes = torch.Tensor(boxes).unsqueeze(0)\n",
    "        resized_image = Normalize(resized_image)\n",
    "        resized_density = torch.from_numpy(resized_density).unsqueeze(0).unsqueeze(0)\n",
    "        sample = {'image':resized_image,'boxes':boxes,'gt_density':resized_density}\n",
    "        return sample\n",
    "\n",
    "\n",
    "Normalize = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IM_NORM_MEAN, std=IM_NORM_STD)])\n",
    "Transform = transforms.Compose([resizeImage( MAX_HW)])\n",
    "TransformTrain = transforms.Compose([resizeImageWithGT(MAX_HW)])\n",
    "\n",
    "\n",
    "def denormalize(tensor, means=IM_NORM_MEAN, stds=IM_NORM_STD):\n",
    "    \"\"\"Reverses the normalisation on a tensor.\n",
    "    Performs a reverse operation on a tensor, so the pixel value range is\n",
    "    between 0 and 1. Useful for when plotting a tensor into an image.\n",
    "    Normalisation: (image - mean) / std\n",
    "    Denormalisation: image * std + mean\n",
    "    Args:\n",
    "        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n",
    "    Shape:\n",
    "        Input: :math:`(N, C, H, W)`\n",
    "        Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Return:\n",
    "        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n",
    "            values between [0, 1]\n",
    "    Note:\n",
    "        Symbols used to describe dimensions:\n",
    "            - N: number of images in a batch\n",
    "            - C: number of channels\n",
    "            - H: height of the image\n",
    "            - W: width of the image\n",
    "    \"\"\"\n",
    "\n",
    "    denormalized = tensor.clone()\n",
    "\n",
    "    for channel, mean, std in zip(denormalized, means, stds):\n",
    "        channel.mul_(std).add_(mean)\n",
    "\n",
    "    return denormalized\n",
    "\n",
    "\n",
    "def scale_and_clip(val, scale_factor, min_val, max_val):\n",
    "    \"Helper function to scale a value and clip it within range\"\n",
    "\n",
    "    new_val = int(round(val*scale_factor))\n",
    "    new_val = max(new_val, min_val)\n",
    "    new_val = min(new_val, max_val)\n",
    "    return new_val\n",
    "\n",
    "\n",
    "def visualize_output_and_save(input_, output, boxes, save_path, figsize=(20, 12), dots=None):\n",
    "    \"\"\"\n",
    "        dots: Nx2 numpy array for the ground truth locations of the dot annotation\n",
    "            if dots is None, this information is not available\n",
    "    \"\"\"\n",
    "\n",
    "    # get the total count\n",
    "    pred_cnt = output.sum().item()\n",
    "    boxes = boxes.squeeze(0)\n",
    "\n",
    "    boxes2 = []\n",
    "    for i in range(0, boxes.shape[0]):\n",
    "        y1, x1, y2, x2 = int(boxes[i, 1].item()), int(boxes[i, 2].item()), int(boxes[i, 3].item()), int(\n",
    "            boxes[i, 4].item())\n",
    "        roi_cnt = output[0,0,y1:y2, x1:x2].sum().item()\n",
    "        boxes2.append([y1, x1, y2, x2, roi_cnt])\n",
    "\n",
    "    img1 = format_for_plotting(denormalize(input_))\n",
    "    output = format_for_plotting(output)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    # display the input image\n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(img1)\n",
    "\n",
    "    for bbox in boxes2:\n",
    "        y1, x1, y2, x2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=3, edgecolor='y', facecolor='none')\n",
    "        rect2 = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='k', linestyle='--', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.add_patch(rect2)\n",
    "\n",
    "    if dots is not None:\n",
    "        ax.scatter(dots[:, 0], dots[:, 1], c='red', edgecolors='blue')\n",
    "        # ax.scatter(dots[:,0], dots[:,1], c='black', marker='+')\n",
    "        ax.set_title(\"Input image, gt count: {}\".format(dots.shape[0]))\n",
    "    else:\n",
    "        ax.set_title(\"Input image\")\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Overlaid result, predicted count: {:.2f}\".format(pred_cnt))\n",
    "\n",
    "    img2 = 0.2989*img1[:,:,0] + 0.5870*img1[:,:,1] + 0.1140*img1[:,:,2]\n",
    "    ax.imshow(img2, cmap='gray')\n",
    "    ax.imshow(output, cmap=plt.cm.viridis, alpha=0.5)\n",
    "\n",
    "\n",
    "    # display the density map\n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Density map, predicted count: {:.2f}\".format(pred_cnt))\n",
    "    ax.imshow(output)\n",
    "    # plt.colorbar()\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 4)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\"Density map, predicted count: {:.2f}\".format(pred_cnt))\n",
    "    ret_fig = ax.imshow(output)\n",
    "    for bbox in boxes2:\n",
    "        y1, x1, y2, x2, roi_cnt = bbox[0], bbox[1], bbox[2], bbox[3], bbox[4]\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=3, edgecolor='y', facecolor='none')\n",
    "        rect2 = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='k', linestyle='--',\n",
    "                                  facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.add_patch(rect2)\n",
    "        ax.text(x1, y1, '{:.2f}'.format(roi_cnt), backgroundcolor='y')\n",
    "\n",
    "    fig.colorbar(ret_fig, ax=ax)\n",
    "\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def format_for_plotting(tensor):\n",
    "    \"\"\"Formats the shape of tensor for plotting.\n",
    "    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
    "    which is not suitable for plotting as images. This function formats an\n",
    "    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n",
    "    data.\n",
    "    Args:\n",
    "        tensor (torch.Tensor, torch.float32): Image tensor\n",
    "    Shape:\n",
    "        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
    "        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n",
    "    Return:\n",
    "        torch.Tensor (torch.float32): Formatted image tensor (detached)\n",
    "    Note:\n",
    "        Symbols used to describe dimensions:\n",
    "            - N: number of images in a batch\n",
    "            - C: number of channels\n",
    "            - H: height of the image\n",
    "            - W: width of the image\n",
    "    \"\"\"\n",
    "\n",
    "    has_batch_dimension = len(tensor.shape) == 4\n",
    "    formatted = tensor.clone()\n",
    "\n",
    "    if has_batch_dimension:\n",
    "        formatted = tensor.squeeze(0)\n",
    "\n",
    "    if formatted.shape[0] == 1:\n",
    "        return formatted.squeeze(0).detach()\n",
    "    else:\n",
    "        return formatted.permute(1, 2, 0).detach()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
